{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Run Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# # Path to the problematic checkpoint\n",
    "# checkpoint_path = r\"c:\\Users\\colin\\OneDrive\\Desktop\\Jersey-Number-Recognition\\data\\pre_trained_models\\reid\\dukemtmcreid_resnet50_256_128_epoch_120.ckpt\"\n",
    "\n",
    "# # Allow loading of ModelCheckpoint objects (since PyTorch 2.6 blocks this by default)\n",
    "# torch.serialization.add_safe_globals([ModelCheckpoint])\n",
    "\n",
    "# # Load the checkpoint safely\n",
    "# checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# # Print out keys to inspect conflicting ones\n",
    "# print(\"Checkpoint keys:\", checkpoint.keys())\n",
    "\n",
    "# # Remove ModelCheckpoint states (if they exist)\n",
    "# keys_to_remove = [key for key in checkpoint.keys() if \"ModelCheckpoint\" in key]\n",
    "# for key in keys_to_remove:\n",
    "#     del checkpoint[key]\n",
    "\n",
    "# # Save the cleaned checkpoint\n",
    "# fixed_checkpoint_path = checkpoint_path.replace(\".ckpt\", \"_fixed.ckpt\")\n",
    "# torch.save(checkpoint, fixed_checkpoint_path)\n",
    "\n",
    "# print(f\"Fixed checkpoint saved to: {fixed_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "print(str(Path.cwd().parent.parent))\n",
    "print(\"Current working directory: \", os.getcwd())\n",
    "\n",
    "from ModelDevelopment.CentralPipeline import CentralPipeline\n",
    "from ModelDevelopment.ImageBatchPipeline import ImageBatchPipeline\n",
    "from DataProcessing.DataPreProcessing import DataPaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreaded Studies Research Findings\n",
    "- ThreadPoolExecutor works best for CentralPipeline pre-processing step (i.e. everything before run_pose)\n",
    "- Time taken to process one batch of 32 tracklets: 251 seconds => (251*32)/3600 = 2.35 hours ETA @ 200 images per batch cap, num_threads = 6x3 = 18\n",
    "- Note on the GPU_SEMAPHORE constant inside ImageFeatureTransformPipeline: this is always capped at 1 so we only offload 1 batch of images to the GPU\n",
    "- Since this gate is capped at 1, we can control the amount of data offloaded to the GPU via the image_batch_size param to CentralPipeline and only that\n",
    "- Since caching is implemented, you can try setting the gate to 2 for even faster results, and if the process crashes, keep restarting with use_cache=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = CentralPipeline(\n",
    "  tracklets_to_process_override=[\"1210\"],\n",
    "  #num_tracklets=1210,\n",
    "  #num_images_per_tracklet=50,\n",
    "  input_data_path=DataPaths.TEST_DATA_DIR.value,\n",
    "  output_processed_data_path=DataPaths.PROCESSED_DATA_OUTPUT_DIR_TEST.value,\n",
    "  common_processed_data_dir=DataPaths.COMMON_PROCESSED_OUTPUT_DATA_TEST.value,\n",
    "  gt_data_path=DataPaths.TEST_DATA_GT.value,\n",
    "  display_transformed_image_sample=False, # NOTE: DO NOT USE. Code is parallelized so we cannot show images anymore. Code breaks, but first one will show if True.\n",
    "  num_image_samples=1,\n",
    "  use_cache=False, # Set to false if you encounter data inconsistencies.\n",
    "  suppress_logging=False,\n",
    "  \n",
    "  # --- PARALLELIZATION PARAMS --- These settings are optimal for an NVIDIA RTX 3070 Ti Laptop GPU.\n",
    "  num_workers=2,            # CRITICAL optimisation param. Adjust accordingly. 6\n",
    "  tracklet_batch_size=32,   # CRITICAL optimisation param. Adjust accordingly. 32\n",
    "  image_batch_size=100,     # CRITICAL optimisation param. Adjust accordingly. 200\n",
    "  num_threads_multiplier=2  # CRITICAL optimisation param. Adjust accordingly. 3. Interpretation: num_threads = num_workers * num_threads_multiplier\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "- Investigate why last tracklet is not generated (1210 is skipped, probably something to do with batching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run_soccernet(\n",
    "  run_soccer_ball_filter=False,\n",
    "  generate_features=False,\n",
    "  run_filter=False,\n",
    "  run_legible=False,\n",
    "  run_legible_eval=False,\n",
    "  run_pose=True,\n",
    "  run_crops=False,\n",
    "  run_str=False,\n",
    "  run_combine=False,\n",
    "  run_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UBC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
