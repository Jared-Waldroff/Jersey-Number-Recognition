{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colin's Pipeline Experiments\n",
    "### Objectives:\n",
    "- Create a SingleImagePipeline so we can run fast-iteration experiments on just a single image\n",
    "- Create a CentraPipeline that calls SingleImagePipeline\n",
    "- Modularize the soccer_net_pipeline and migrate all operations to SingleImagePipeline\n",
    "- Run all systematic ops from CentralPipeline (more than one tracklet)\n",
    "\n",
    "### Main benefit from creating a `SingleImagePipeline` and a `CentralPipeline`\n",
    "Decoupling large-scale systematic training versus testing. The main pipeline trains the model, creates output processed images and does so by systematically traversing all the tracklets. I want to be able to see what pre-processing is happening to the image so I can understand what is being fed to the model. I also just want to be able to pass a single raw image to the model so I can get fast results. Right now everything is coupled together by all the tracklets, and I don't want to traverse all of them or even a single tracklet, but maybe only a single image from a single tracklet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Run Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install yacs\n",
    "#%pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\n",
      "Current working directory:  c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\ModelDevelopment\\experiments\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "print(str(Path.cwd().parent.parent))\n",
    "print(\"Current working directory: \", os.getcwd())\n",
    "\n",
    "from ModelDevelopment.CentralPipeline import CentralPipeline\n",
    "from ModelDevelopment.SingleImagePipeline import SingleImagePipeline\n",
    "from DataProcessing.DataPreProcessing import DataPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-03 20:17:25] - [INFO]: DataPreProcessing initialized. Universe of available data paths:\n",
      "[2025-03-03 20:17:25] - [INFO]: ROOT_DATA_DIR: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\SoccerNet\\jersey-2023\\extracted\n",
      "[2025-03-03 20:17:25] - [INFO]: TEST_DATA_DIR: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\SoccerNet\\jersey-2023\\extracted\\test\\images\n",
      "[2025-03-03 20:17:25] - [INFO]: TRAIN_DATA_DIR: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\SoccerNet\\jersey-2023\\extracted\\train\\images\n",
      "[2025-03-03 20:17:25] - [INFO]: CHALLENGE_DATA_DIR: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\SoccerNet\\jersey-2023\\extracted\\challenge\\images\n",
      "[2025-03-03 20:17:25] - [INFO]: PRE_TRAINED_MODELS_DIR: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\pre_trained_models\n",
      "[2025-03-03 20:17:25] - [INFO]: REID_PRE_TRAINED: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\pre_trained_models\\reid\n",
      "[2025-03-03 20:17:25] - [INFO]: REID_MODEL_1: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\pre_trained_models\\reid\\dukemtmcreid_resnet50_256_128_epoch_120.ckpt\n",
      "[2025-03-03 20:17:25] - [INFO]: REID_MODEL_2: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\pre_trained_models\\reid\\market1501_resnet50_256_128_epoch_120.ckpt\n",
      "[2025-03-03 20:17:25] - [INFO]: REID_CONFIG_YAML: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\pre_trained_models\\reid\\configs\\256_resnet50.yml\n",
      "[2025-03-03 20:17:25] - [INFO]: PROCESSED_DATA_OUTPUT_DIR: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\SoccerNet\\jersey-2023\\processed_data\n",
      "[2025-03-03 20:17:25] - [INFO]: PROCESSED_DATA_OUTPUT_DIR_TRAIN: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\SoccerNet\\jersey-2023\\processed_data\\train\n",
      "[2025-03-03 20:17:25] - [INFO]: PROCESSED_DATA_OUTPUT_DIR_TEST: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\SoccerNet\\jersey-2023\\processed_data\\test\n",
      "[2025-03-03 20:17:25] - [INFO]: PROCESSED_DATA_OUTPUT_DIR_CHALLENGE: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\SoccerNet\\jersey-2023\\processed_data\\challenge\n",
      "[2025-03-03 20:17:25] - [INFO]: Using device: cuda\n",
      "[2025-03-03 20:17:25] - [INFO]: ['0', '1', '10', '100', '1000', '1001', '1002', '1003', '1004', '1005']\n",
      "[2025-03-03 20:17:25] - [INFO]: Min tracklet: 0\n",
      "[2025-03-03 20:17:25] - [INFO]: Max tracklet: 1426\n"
     ]
    }
   ],
   "source": [
    "pipeline = CentralPipeline(\n",
    "  input_data_path=DataPaths.TRAIN_DATA_DIR.value,\n",
    "  output_processed_data_path=DataPaths.PROCESSED_DATA_OUTPUT_DIR_TRAIN.value\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\colin\\miniconda3\\envs\\UBC\\Lib\\site-packages\\pytorch_lightning\\utilities\\upgrade_checkpoint.py\", line 101, in <module>\n",
      "    main()\n",
      "    ~~~~^^\n",
      "  File \"c:\\Users\\colin\\miniconda3\\envs\\UBC\\Lib\\site-packages\\pytorch_lightning\\utilities\\upgrade_checkpoint.py\", line 97, in main\n",
      "    _upgrade(args)\n",
      "    ~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\colin\\miniconda3\\envs\\UBC\\Lib\\site-packages\\pytorch_lightning\\utilities\\upgrade_checkpoint.py\", line 62, in _upgrade\n",
      "    checkpoint = torch.load(file, map_location=(torch.device(\"cpu\") if args.map_to_cpu else None))\n",
      "  File \"c:\\Users\\colin\\miniconda3\\envs\\UBC\\Lib\\site-packages\\torch\\serialization.py\", line 1470, in load\n",
      "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
      "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint was not an allowed global by default. Please use `torch.serialization.add_safe_globals([ModelCheckpoint])` or the `torch.serialization.safe_globals([ModelCheckpoint])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-03 20:17:26] - [INFO]: Running the SoccerNet pipeline.\n",
      "[2025-03-03 20:17:26] - [INFO]: num_tracklets: 1\n",
      "[2025-03-03 20:17:26] - [INFO]: Using single-process GPU mode to generate features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d28baeadf24f74bb861b28070bbae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading tracklets (GPU):   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\colin\\miniconda3\\envs\\UBC\\Lib\\site-packages\\pytorch_lightning\\utilities\\migration\\migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.1.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\pre_trained_models\\reid\\dukemtmcreid_resnet50_256_128_epoch_120.ckpt`\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "property 'hparams' of 'CTLModel' object has no setter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_soccernet_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDataPaths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPROCESSED_DATA_OUTPUT_DIR_TRAIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnum_tracklets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnum_images_per_tracklet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdisplay_transformed_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\ModelDevelopment\\CentralPipeline.py:102\u001b[0m, in \u001b[0;36mCentralPipeline.run_soccernet_pipeline\u001b[1;34m(self, output_folder, num_tracklets, num_images_per_tracklet, display_transformed_image)\u001b[0m\n\u001b[0;32m     99\u001b[0m display_transformed_image \u001b[38;5;241m=\u001b[39m num_images \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDISP_IMAGE_CAP\n\u001b[0;32m    100\u001b[0m num_images \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 102\u001b[0m single_image_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mSingleImagePipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m  \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModelUniverse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDUMMY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m  \u001b[49m\u001b[43msilence_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdisplay_transformed_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_transformed_image\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# This run_model call is where we actually do things.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# It is the full end-to-end soccer net pipeline from main.py, but heavily modularized.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# This is also where we would adjust the pipeline with our own improvements.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# If so, we would like to augment this class with more images during training,\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# but during testing, fetch the retrieval module.\u001b[39;00m\n\u001b[0;32m    117\u001b[0m single_image_pipeline\u001b[38;5;241m.\u001b[39mrun_model_chain()\n",
      "File \u001b[1;32mc:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\ModelDevelopment\\SingleImagePipeline.py:23\u001b[0m, in \u001b[0;36mSingleImagePipeline.__init__\u001b[1;34m(self, raw_image_tensor, model, silence_logs, display_transformed_image)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m CustomLogger()\u001b[38;5;241m.\u001b[39mget_logger()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Pass the raw_image_tensor through the preprocessing pipeline\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_preprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_image_transform_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_image_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_image_tensor\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_augmentation \u001b[38;5;241m=\u001b[39m DataAugmentation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed_image)\n",
      "File \u001b[1;32mc:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\DataProcessing\\DataPreProcessing.py:117\u001b[0m, in \u001b[0;36mDataPreProcessing.single_image_transform_pipeline\u001b[1;34m(self, raw_image, model_version)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msingle_image_transform_pipeline\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_image, model_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mres50_market\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# Step 2: Pass through the centroid model that:\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m#         1. Resizes + crops the image\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# Step 1 tranform the image using the reid centroid model\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     processed_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpass_through_reid_centroid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_version\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\DataProcessing\\DataPreProcessing.py:85\u001b[0m, in \u001b[0;36mDataPreProcessing.pass_through_reid_centroid\u001b[1;34m(self, raw_image, model_version)\u001b[0m\n\u001b[0;32m     82\u001b[0m cfg\u001b[38;5;241m.\u001b[39mmerge_from_list(opts)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# cfg.MODEL.PRETRAIN_PATH = \"\" inside that cfg...\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCTLModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRETRAIN_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cuda:\n\u001b[0;32m     88\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\colin\\miniconda3\\envs\\UBC\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m     )\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\colin\\miniconda3\\envs\\UBC\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1581\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1579\u001b[0m \n\u001b[0;32m   1580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1581\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[1;32mc:\\Users\\colin\\miniconda3\\envs\\UBC\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:91\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m---> 91\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[1;32mc:\\Users\\colin\\miniconda3\\envs\\UBC\\Lib\\site-packages\\pytorch_lightning\\core\\saving.py:165\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cls_spec\u001b[38;5;241m.\u001b[39mvarkw:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# filter kwargs according to class init unless it allows any argument via kwargs\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     _cls_kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _cls_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m cls_init_args_name}\n\u001b[1;32m--> 165\u001b[0m obj \u001b[38;5;241m=\u001b[39m instantiator(\u001b[38;5;28mcls\u001b[39m, _cls_kwargs) \u001b[38;5;28;01mif\u001b[39;00m instantiator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_cls_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, pl\u001b[38;5;241m.\u001b[39mLightningDataModule):\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint:\n",
      "File \u001b[1;32mc:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\reid\\CentroidsReidRepo\\train_ctl_model.py:29\u001b[0m, in \u001b[0;36mCTLModel.__init__\u001b[1;34m(self, cfg, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_xent\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_triplet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_center\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcentroid_triplet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     ]\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses_dict \u001b[38;5;241m=\u001b[39m {n: [] \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses_names}\n",
      "File \u001b[1;32mc:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\reid\\CentroidsReidRepo\\modelling\\bases.py:63\u001b[0m, in \u001b[0;36mModelBase.__init__\u001b[1;34m(self, cfg, test_dataloader, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mTEST\u001b[38;5;241m.\u001b[39mONLY_TEST:\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# To make sure that loaded hparams are overwritten by cfg we may have chnaged\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         hparams \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg}\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m \u001b[38;5;241m=\u001b[39m AttributeDict(hparams)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_dataloader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\colin\\miniconda3\\envs\\UBC\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2029\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   2026\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(name, value)\n\u001b[0;32m   2027\u001b[0m     \u001b[38;5;66;03m# === HACK END ===\u001b[39;00m\n\u001b[0;32m   2028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2029\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: property 'hparams' of 'CTLModel' object has no setter"
     ]
    }
   ],
   "source": [
    "pipeline.run_soccernet_pipeline(\n",
    "  output_folder=DataPaths.PROCESSED_DATA_OUTPUT_DIR_TRAIN.value,\n",
    "  num_tracklets=1,\n",
    "  num_images_per_tracklet=1,\n",
    "  display_transformed_image=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UBC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
