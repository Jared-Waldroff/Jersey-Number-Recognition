{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colin's active development sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-26 10:31:32] - [INFO]: ROOT_DATA_DIR: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\SoccerNet\\jersey-2023\\extracted\n",
      "[2025-02-26 10:31:32] - [INFO]: TRAIN_DATA_DIR: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\SoccerNet\\jersey-2023\\extracted\\train\\images\n",
      "[2025-02-26 10:31:32] - [INFO]: TEST_DATA_DIR: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\SoccerNet\\jersey-2023\\extracted\\test\\images\n",
      "[2025-02-26 10:31:32] - [INFO]: VAL_DATA_DIR: c:\\Users\\colin\\OneDrive\\Desktop\\UBC\\Jersey-Number-Recognition\\data\\SoccerNet\\jersey-2023\\extracted\\challenge\\images\n",
      "[2025-02-26 10:31:32] - [INFO]: Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s] - [%(levelname)s]: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "class DataPaths(Enum):\n",
    "    ROOT_DATA_DIR = str(Path.cwd().parent / 'data' / 'SoccerNet' / 'jersey-2023' / 'extracted')\n",
    "    TEST_DATA_DIR = str(Path(ROOT_DATA_DIR) / 'test' / 'images')\n",
    "    TRAIN_DATA_DIR = str(Path(ROOT_DATA_DIR) / 'train' / 'images')\n",
    "    VALIDATION_DATA_DIR = str(Path(ROOT_DATA_DIR) / 'challenge' / 'images')\n",
    "    TEMP_EXPERIMENT_DIR = str(Path.cwd() / 'experiments' / 'temp')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info(f\"ROOT_DATA_DIR: {DataPaths.ROOT_DATA_DIR.value}\")\n",
    "logging.info(f\"TRAIN_DATA_DIR: {DataPaths.TRAIN_DATA_DIR.value}\")\n",
    "logging.info(f\"TEST_DATA_DIR: {DataPaths.TEST_DATA_DIR.value}\")\n",
    "logging.info(f\"VAL_DATA_DIR: {DataPaths.VALIDATION_DATA_DIR.value}\")\n",
    "logging.info(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracklet iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the .DS_Store files\n",
    "\n",
    "def get_tracks(input_folder):\n",
    "    tracks = [t for t in os.listdir(input_folder) if not t.startswith('.')]\n",
    "    logging.info(tracks[0:10])\n",
    "\n",
    "    # Extract numerical part and convert to integer for comparison\n",
    "    def extract_number(track):\n",
    "        match = re.search(r'(\\d+)', track)  # Extracts the first sequence of digits\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return -1  # Provide a default value if no number is found\n",
    "\n",
    "    # Find min and max tracklets based on the extracted number\n",
    "    if tracks:\n",
    "        min_track = min(tracks, key=extract_number)\n",
    "        max_track = max(tracks, key=extract_number)\n",
    "\n",
    "        logging.info(f\"Min tracklet: {min_track}\")\n",
    "        logging.info(f\"Max tracklet: {max_track}\")\n",
    "    else:\n",
    "        logging.warning(\"No tracklets found.\")\n",
    "        \n",
    "    return tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(input_folder, output_folder, model_version='res50_market', load_only=False):\n",
    "    \"\"\"\n",
    "    If load_only is True, simply load images, apply transforms, and return the tensors.\n",
    "    Otherwise, run them through the model backbone and return the extracted features.\n",
    "    \"\"\"\n",
    "    if not load_only:\n",
    "        # Load model if we're going to extract features\n",
    "        CONFIG_FILE, MODEL_FILE = get_specs_from_version(model_version)\n",
    "        cfg.merge_from_file(CONFIG_FILE)\n",
    "        opts = [\"MODEL.PRETRAIN_PATH\", MODEL_FILE, \"MODEL.PRETRAINED\", True,\n",
    "                \"TEST.ONLY_TEST\", True, \"MODEL.RESUME_TRAINING\", False]\n",
    "        cfg.merge_from_list(opts)\n",
    "        use_cuda = True if torch.cuda.is_available() and cfg.GPU_IDS else False\n",
    "        model = CTLModel.load_from_checkpoint(cfg.MODEL.PRETRAIN_PATH, cfg=cfg)\n",
    "        if use_cuda:\n",
    "            model.to('cuda')\n",
    "            logging.info(\"using GPU\")\n",
    "        model.eval()\n",
    "    else:\n",
    "        use_cuda = False  # No model inference when load_only is True\n",
    "\n",
    "    # Define validation transforms using torchvision\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Get list of valid track directories (skip hidden files)\n",
    "    tracks = get_tracks(DataPaths.TRAIN_DATA_DIR.value)[0]\n",
    "    \n",
    "    # Dictionary to store processed data (per track)\n",
    "    processed_data = {}\n",
    "    for track in tqdm(tracks, desc=\"Processing tracks\"):\n",
    "        track_path = os.path.normpath(os.path.join(input_folder, track))\n",
    "        if not os.path.isdir(track_path):\n",
    "            continue\n",
    "        images = [img for img in os.listdir(track_path) if not img.startswith('.')]\n",
    "        track_features = []\n",
    "        for img_path in images:\n",
    "            img_full_path = os.path.normpath(os.path.join(track_path, img_path))\n",
    "            try:\n",
    "                # Load image using cv2 then convert to PIL\n",
    "                img = cv2.imread(img_full_path)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                input_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "                # Apply the validation transforms to get a tensor\n",
    "                transformed = val_transforms(input_img)  # returns a tensor\n",
    "                if load_only:\n",
    "                    # Simply store the tensor (add a batch dimension for later concatenation)\n",
    "                    track_features.append(transformed.unsqueeze(0))\n",
    "                else:\n",
    "                    # If not load_only, run through model to extract features\n",
    "                    input_tensor = torch.stack([transformed])\n",
    "                    with torch.no_grad():\n",
    "                        _, global_feat = model.backbone(input_tensor.cuda() if use_cuda else input_tensor)\n",
    "                        global_feat = model.bn(global_feat)\n",
    "                    # Flatten and convert to numpy\n",
    "                    track_features.append(global_feat.cpu().numpy().reshape(-1,))\n",
    "            except Exception as e:\n",
    "                logging.info(f\"Error processing {img_full_path}: {e}\")\n",
    "                continue\n",
    "        if track_features:\n",
    "            if load_only:\n",
    "                # Concatenate tensors along the batch dimension\n",
    "                processed_data[track] = torch.cat(track_features, dim=0)\n",
    "            else:\n",
    "                processed_data[track] = np.array(track_features)\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-26 10:31:32] - [INFO]: ['0', '1', '10', '100', '1000', '1001', '1002', '1003', '1004', '1005']\n",
      "[2025-02-26 10:31:32] - [INFO]: Min tracklet: 0\n",
      "[2025-02-26 10:31:32] - [INFO]: Max tracklet: 1426\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd90a1d4d229419d8065633a5f4ba33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing tracks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_folder = DataPaths.TRAIN_DATA_DIR.value\n",
    "\n",
    "# Call generate_features with load_only=True to simply load the images as tensors\n",
    "data_dict = generate_features(input_folder, DataPaths.TEMP_EXPERIMENT_DIR.value, load_only=True)\n",
    "\n",
    "# Inspect the loaded data\n",
    "for track, tensor in data_dict.items():\n",
    "    logging.info(f\"Track: {track}, Loaded Tensor Shape: {tensor.shape}\")\n",
    "\n",
    "# Visualize a sample image from one track\n",
    "import matplotlib.pyplot as plt\n",
    "sample_track = list(data_dict.keys())[0]\n",
    "sample_tensor = data_dict[sample_track][0]  # take the first image in the track\n",
    "# Convert tensor to numpy and show it\n",
    "plt.imshow(sample_tensor.permute(1, 2, 0).numpy())\n",
    "plt.title(f\"Sample from track: {sample_track}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UBC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
